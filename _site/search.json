[
  {
    "objectID": "Prac_One_IFRFAB001.html",
    "href": "Prac_One_IFRFAB001.html",
    "title": "Practical One",
    "section": "",
    "text": "Find all rows in “airquality” that have missing values.\n\nThis was completed by using the complete.cases() function. The complete.cases() function checks to see if the rows have missing (NA) values returning TRUE if there are no missing values and FALSE otherwise.\n\ndata(airquality) #load the data to see it better\n\nmissing_rows &lt;- airquality[!complete.cases(airquality), ] #Find all NA rows\nmissing_rows_numbers &lt;- as.character(rownames(missing_rows)) #Stores as numbers \nprint(missing_rows_numbers) # Prints to display \n\n [1] \"5\"   \"6\"   \"10\"  \"11\"  \"25\"  \"26\"  \"27\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n[13] \"37\"  \"39\"  \"42\"  \"43\"  \"45\"  \"46\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\" \n[25] \"58\"  \"59\"  \"60\"  \"61\"  \"65\"  \"72\"  \"75\"  \"83\"  \"84\"  \"96\"  \"97\"  \"98\" \n[37] \"102\" \"103\" \"107\" \"115\" \"119\" \"150\"\n\n\nThe code outputs the row numbers of the rows with missing values\n\n\n\n\nFind mean, sd, min, max for each of temperature and ozone level.\n\nThis was accomplished in 4 steps:\n\nCreate vectors containing valid Ozone and Temperature values\nUse the built in mean(), sd(), min() and max() functions on these vectors\nCreate vectors for each set of statistics\nCreate a data frame using these vectors to be displayed\n\n\n#Step 1\nvalid_ozone &lt;- airquality$Ozone[complete.cases(airquality$Ozone)]\nvalid_temp &lt;- airquality$Temp[complete.cases(airquality$Temp)]\n\n#Step 2\nmean_ozone &lt;- mean(valid_ozone)\nsd_ozone &lt;- sd(valid_ozone)\nmin_ozone &lt;- min(valid_ozone)\nmax_ozone &lt;- max(valid_ozone)\n\nmean_temp &lt;- mean(valid_temp)\nsd_temp &lt;- sd(valid_temp)\nmin_temp &lt;- min(valid_temp)\nmax_temp &lt;- max(valid_temp)\n\n#Step 3\nozone_stats &lt;- c(mean_ozone, sd_ozone, min_ozone, max_ozone) #Vector for df\ntemp_stats &lt;- c(mean_temp, sd_temp, min_temp, max_temp) #Vector for df\n\n#Step 4\ndf &lt;- data.frame(ozone_stats, temp_stats)\nrownames(df) &lt;- c(\"Mean\", \"SD\", \"Min\", \"Max\")\ncolnames(df) &lt;- c(\"Ozone\", \"Temperature\")\nprint(df)\n\n         Ozone Temperature\nMean  42.12931    77.88235\nSD    32.98788     9.46527\nMin    1.00000    56.00000\nMax  168.00000    97.00000\n\n\n\n\n\n\nFor linear regression, parameter estimates can be found as follows.\n\\[\n\\hat{\\beta} = (X^TX)^{-1}X^TY\n\\]Here, Y is the response variable, and X is the design matrix. The cars data (an R data set, also always available in R) contains two variables: speed and distance to stop. Fit a simple linear regression model to these data, i.e. find the estimates, using the equation above, and matrix calculations in R.\n\nOur goal is to estimate the parameters of the regression line using matrix operations rather than the built-in lm() function using R.\nI created a function called matrix_regression it takes in 2 parameters, a formula and a data set and returns a data frame of the regression summary statistics. The function computes the \\(\\hat{\\beta}\\) estimates, standard error, RSE, test statistic and p value of the intercept and slope.\nThe function follows the following steps:\n\nAssign values of the Y vector and X design matrix from the equation\nCompute \\(X^TX\\)\nCompute the inverse of \\(X^TX\\), \\((X^TX)^{-1}\\)\nCompute \\(X^TY\\)\nUse the formula with the computed values to get \\(\\hat{\\beta}\\)\nCompute the RSE, standard error, test statistic and p-value in that order\n\n\n############## CARS DATASET ###############\n##### SETUP #####\ndata(cars)\nclass(cars)\n\n[1] \"data.frame\"\n\n##### FUNCTION ######\n\nmatrix_regression &lt;- function(formula, data){\n\n##### VARIABLE DECLARATIONS #####\ny_vec &lt;- model.response(model.frame(formula, data)) #dependent\nx_matrix &lt;- model.matrix(formula, data) #design matrix\n\n\n##### COMPUTING X^TX #####\nxtx_matrix &lt;- t(x_matrix) %*% x_matrix #transpose then matrix multiply\n\n##### INVERSE #####\ninv_matrix &lt;- solve(xtx_matrix) #no b value therefore solves for inverse\n\n##### COMPUTING X^TY #####\nxty &lt;- t(x_matrix) %*% y_vec\n\n##### PARAMETER ESTIMATION USING GIVEN FORMULA #####\nbeta &lt;- inv_matrix %*% xty\n\ny_hat &lt;- x_matrix %*% beta\nresids &lt;- y_hat - y_vec\n\n##### RESIDUAL VARIANCE #####\nn_obs &lt;- nrow(data)\np &lt;- ncol(x_matrix)\nresvar &lt;- sum(resids^2)/(n_obs-p)\nsigma &lt;- sqrt(resvar)\nvar_covar_matrix &lt;- resvar * inv_matrix\n\n##### STANDARD ERROR #####\nstderr &lt;- sqrt(diag(var_covar_matrix))\n\n##### T STAT #####\nt_stat &lt;- beta/stderr\n\n##### P-VALUES #####\np_val &lt;- 2*(1-pt(abs(t_stat), df = n_obs - p))\n\n##### DATAFRAME FOR PRINTING #####\nresults &lt;- data.frame(row.names = c(\"(Intercept)\", \"Speed\"), \n                      Estimate = beta, \n                      Std_Error = stderr,\n                      RSE = sigma,\n                      t_statistic = t_stat, \n                      p_value = p_val)\nreturn(results)\n\n}\n\nmat_reg &lt;- matrix_regression(dist~speed, cars)\n\nprint(mat_reg)\n\n              Estimate Std_Error      RSE t_statistic      p_value\n(Intercept) -17.579095 6.7584402 15.37959   -2.601058 1.231882e-02\nSpeed         3.932409 0.4155128 15.37959    9.463990 1.489919e-12\n\n\n\n\n\n\nCheck that you get the same \\(\\beta\\) estimates as when fitting the linear regression model using lm() in R.\n\nThis is self explanatory, we will just use the lm() function to get the \\(\\beta\\) estimates, we then use the summary() function to get the remaining statistics for comparison.\n\nlin_reg_model = lm(dist ~ speed, data = cars)\nprint(lin_reg_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\nsummary(lin_reg_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nIt can be seen that the statistics obtained using the built-in lm() function are the same as those obtained using our custom function."
  },
  {
    "objectID": "Prac_One_IFRFAB001.html#task-1-missing-values",
    "href": "Prac_One_IFRFAB001.html#task-1-missing-values",
    "title": "Practical One",
    "section": "",
    "text": "Find all rows in “airquality” that have missing values.\n\nThis was completed by using the complete.cases() function. The complete.cases() function checks to see if the rows have missing (NA) values returning TRUE if there are no missing values and FALSE otherwise.\n\ndata(airquality) #load the data to see it better\n\nmissing_rows &lt;- airquality[!complete.cases(airquality), ] #Find all NA rows\nmissing_rows_numbers &lt;- as.character(rownames(missing_rows)) #Stores as numbers \nprint(missing_rows_numbers) # Prints to display \n\n [1] \"5\"   \"6\"   \"10\"  \"11\"  \"25\"  \"26\"  \"27\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n[13] \"37\"  \"39\"  \"42\"  \"43\"  \"45\"  \"46\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\" \n[25] \"58\"  \"59\"  \"60\"  \"61\"  \"65\"  \"72\"  \"75\"  \"83\"  \"84\"  \"96\"  \"97\"  \"98\" \n[37] \"102\" \"103\" \"107\" \"115\" \"119\" \"150\"\n\n\nThe code outputs the row numbers of the rows with missing values"
  },
  {
    "objectID": "Prac_One_IFRFAB001.html#task-2-statistical-analysis-of-ozone-and-temperature",
    "href": "Prac_One_IFRFAB001.html#task-2-statistical-analysis-of-ozone-and-temperature",
    "title": "Practical One",
    "section": "",
    "text": "Find mean, sd, min, max for each of temperature and ozone level.\n\nThis was accomplished in 4 steps:\n\nCreate vectors containing valid Ozone and Temperature values\nUse the built in mean(), sd(), min() and max() functions on these vectors\nCreate vectors for each set of statistics\nCreate a data frame using these vectors to be displayed\n\n\n#Step 1\nvalid_ozone &lt;- airquality$Ozone[complete.cases(airquality$Ozone)]\nvalid_temp &lt;- airquality$Temp[complete.cases(airquality$Temp)]\n\n#Step 2\nmean_ozone &lt;- mean(valid_ozone)\nsd_ozone &lt;- sd(valid_ozone)\nmin_ozone &lt;- min(valid_ozone)\nmax_ozone &lt;- max(valid_ozone)\n\nmean_temp &lt;- mean(valid_temp)\nsd_temp &lt;- sd(valid_temp)\nmin_temp &lt;- min(valid_temp)\nmax_temp &lt;- max(valid_temp)\n\n#Step 3\nozone_stats &lt;- c(mean_ozone, sd_ozone, min_ozone, max_ozone) #Vector for df\ntemp_stats &lt;- c(mean_temp, sd_temp, min_temp, max_temp) #Vector for df\n\n#Step 4\ndf &lt;- data.frame(ozone_stats, temp_stats)\nrownames(df) &lt;- c(\"Mean\", \"SD\", \"Min\", \"Max\")\ncolnames(df) &lt;- c(\"Ozone\", \"Temperature\")\nprint(df)\n\n         Ozone Temperature\nMean  42.12931    77.88235\nSD    32.98788     9.46527\nMin    1.00000    56.00000\nMax  168.00000    97.00000"
  },
  {
    "objectID": "Prac_One_IFRFAB001.html#task-3-linear-modelling-without-using-lm",
    "href": "Prac_One_IFRFAB001.html#task-3-linear-modelling-without-using-lm",
    "title": "Practical One",
    "section": "",
    "text": "For linear regression, parameter estimates can be found as follows.\n\\[\n\\hat{\\beta} = (X^TX)^{-1}X^TY\n\\]Here, Y is the response variable, and X is the design matrix. The cars data (an R data set, also always available in R) contains two variables: speed and distance to stop. Fit a simple linear regression model to these data, i.e. find the estimates, using the equation above, and matrix calculations in R.\n\nOur goal is to estimate the parameters of the regression line using matrix operations rather than the built-in lm() function using R.\nI created a function called matrix_regression it takes in 2 parameters, a formula and a data set and returns a data frame of the regression summary statistics. The function computes the \\(\\hat{\\beta}\\) estimates, standard error, RSE, test statistic and p value of the intercept and slope.\nThe function follows the following steps:\n\nAssign values of the Y vector and X design matrix from the equation\nCompute \\(X^TX\\)\nCompute the inverse of \\(X^TX\\), \\((X^TX)^{-1}\\)\nCompute \\(X^TY\\)\nUse the formula with the computed values to get \\(\\hat{\\beta}\\)\nCompute the RSE, standard error, test statistic and p-value in that order\n\n\n############## CARS DATASET ###############\n##### SETUP #####\ndata(cars)\nclass(cars)\n\n[1] \"data.frame\"\n\n##### FUNCTION ######\n\nmatrix_regression &lt;- function(formula, data){\n\n##### VARIABLE DECLARATIONS #####\ny_vec &lt;- model.response(model.frame(formula, data)) #dependent\nx_matrix &lt;- model.matrix(formula, data) #design matrix\n\n\n##### COMPUTING X^TX #####\nxtx_matrix &lt;- t(x_matrix) %*% x_matrix #transpose then matrix multiply\n\n##### INVERSE #####\ninv_matrix &lt;- solve(xtx_matrix) #no b value therefore solves for inverse\n\n##### COMPUTING X^TY #####\nxty &lt;- t(x_matrix) %*% y_vec\n\n##### PARAMETER ESTIMATION USING GIVEN FORMULA #####\nbeta &lt;- inv_matrix %*% xty\n\ny_hat &lt;- x_matrix %*% beta\nresids &lt;- y_hat - y_vec\n\n##### RESIDUAL VARIANCE #####\nn_obs &lt;- nrow(data)\np &lt;- ncol(x_matrix)\nresvar &lt;- sum(resids^2)/(n_obs-p)\nsigma &lt;- sqrt(resvar)\nvar_covar_matrix &lt;- resvar * inv_matrix\n\n##### STANDARD ERROR #####\nstderr &lt;- sqrt(diag(var_covar_matrix))\n\n##### T STAT #####\nt_stat &lt;- beta/stderr\n\n##### P-VALUES #####\np_val &lt;- 2*(1-pt(abs(t_stat), df = n_obs - p))\n\n##### DATAFRAME FOR PRINTING #####\nresults &lt;- data.frame(row.names = c(\"(Intercept)\", \"Speed\"), \n                      Estimate = beta, \n                      Std_Error = stderr,\n                      RSE = sigma,\n                      t_statistic = t_stat, \n                      p_value = p_val)\nreturn(results)\n\n}\n\nmat_reg &lt;- matrix_regression(dist~speed, cars)\n\nprint(mat_reg)\n\n              Estimate Std_Error      RSE t_statistic      p_value\n(Intercept) -17.579095 6.7584402 15.37959   -2.601058 1.231882e-02\nSpeed         3.932409 0.4155128 15.37959    9.463990 1.489919e-12"
  },
  {
    "objectID": "Prac_One_IFRFAB001.html#task-4-comparison-with-lm",
    "href": "Prac_One_IFRFAB001.html#task-4-comparison-with-lm",
    "title": "Practical One",
    "section": "",
    "text": "Check that you get the same \\(\\beta\\) estimates as when fitting the linear regression model using lm() in R.\n\nThis is self explanatory, we will just use the lm() function to get the \\(\\beta\\) estimates, we then use the summary() function to get the remaining statistics for comparison.\n\nlin_reg_model = lm(dist ~ speed, data = cars)\nprint(lin_reg_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\nsummary(lin_reg_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nIt can be seen that the statistics obtained using the built-in lm() function are the same as those obtained using our custom function."
  },
  {
    "objectID": "Prac2.html",
    "href": "Prac2.html",
    "title": "Prac2",
    "section": "",
    "text": "This practical revolves around implemented a custom Loews smoothing function and comparing the output with the integrated lowess() function.\n\n\n\n\n\n\nset.seed(1) #Set your seed to 1\n\n# Create X as a sequence of numbers from 1 to 100 \nx_vec &lt;- numeric(100) #allocate space\nfor (i in 1:100){\n  x_vec[i] &lt;- i\n}\n\ne_vec &lt;- rnorm(100, mean = 0, sd = 0.2) #Generates 100 error terms\n\n# Generate Y as a noisy sine wave according to equation\ny_vec &lt;- numeric(100) #allocate space\nfor (i in 1:100){\n  y_vec[i]&lt;- sin(x_vec[i]/10) + e_vec[i] #equation given\n}\n\n\n\n\n\n\ncustomLowess &lt;- function(x, y, f){\n  # Data \n  n &lt;- length(x)\n  # Span\n  k &lt;- f*n\n  # Smoothed weights\n  returnable &lt;- numeric(n)\n  \n  for(i in 1:n){ #repeat for all x_i\n    distances &lt;- abs(x-x[i]) #get distances of neighbours\n    neighbours_ordered &lt;- order(distances)[1:k] #stores k closest neighbours indices\n    d_max &lt;- distances[neighbours_ordered[k]] #furthest = last in ordered vector\n    weights &lt;- (1-(distances[neighbours_ordered]/d_max)^3)^3 #Tricube\n    \n    #Weighted Regression\n    neighbour_x &lt;- x[neighbours_ordered]\n    neighbour_y &lt;- y[neighbours_ordered]\n    weight_diag &lt;- diag(weights) #makes a diagonal matrix of the weights\n    design_matrix &lt;- cbind(1, neighbour_x)\n    beta &lt;- solve(\n      t(design_matrix)%*%weight_diag%*%design_matrix\n      )%*%\n      t(design_matrix)%*%weight_diag%*%neighbour_y\n    \n    #Smoothed value\n    smoothed_y &lt;- beta[1]+beta[2]*x[i]\n    returnable[i] &lt;- smoothed_y\n  }\n  \n  return(returnable)\n}\n\nsmoothed &lt;- numeric(100)\nsmoothed &lt;- customLowess(x_vec, y_vec, 0.1)\n\n\n\n\nI made use of the ggplot2 package to create plots of the data and compare them side-by-side as seen below. I needed to install and use the patchwork package for the quarto rendering.\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\nlibrary(ggplot2)\ninstall.packages(\"patchwork\")\n\npackage 'patchwork' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\fabio\\AppData\\Local\\Temp\\RtmpmEDkZY\\downloaded_packages\n\nlibrary(patchwork)\n\nsmoothed &lt;- numeric(100)\nsmoothed &lt;- customLowess(x_vec, y_vec, 0.1)\n\n#makes it the smoothed y-values as it returns \n#both x and y by default\nbuilt_in_smooth &lt;- lowess(x_vec, y_vec, 0.1, 0)$y \n\n#Plot CUSTOM\ncustom_plot &lt;- ggplot(\n  data.frame(\n    x_vec = x_vec, \n    y_vec = y_vec, \n    smoothed = smoothed), \n  aes(x = x_vec, y = y_vec)) + \n  geom_point(color = \"gray\") + \n  geom_line(\n    aes(y = smoothed), \n    color = \"red\", \n    lwd = 1) + ggtitle(\"Custom Function\")\n\n#Plot BUILT-IN\nbuilt_in_plot &lt;- ggplot(\n  data.frame(\n    x_vec = x_vec, \n    y_vec = y_vec, \n    built_in_smooth = built_in_smooth), \n  aes(x = x_vec, y = y_vec)) + \n  geom_point(color = \"gray\") + \n  geom_line(\n    aes(y = built_in_smooth), \n    color = \"green\", \n    lwd = 1) + ggtitle(\"Built-in Function\")\n\nfinal_plot &lt;- custom_plot + built_in_plot #displays them side-by-side\nfinal_plot\n\n\n\n\n\n\n\n\nAs you can see, the 2 plots are identical. Thus our custom function is working correctly."
  },
  {
    "objectID": "Prac2.html#data",
    "href": "Prac2.html#data",
    "title": "Prac2",
    "section": "",
    "text": "set.seed(1) #Set your seed to 1\n\n# Create X as a sequence of numbers from 1 to 100 \nx_vec &lt;- numeric(100) #allocate space\nfor (i in 1:100){\n  x_vec[i] &lt;- i\n}\n\ne_vec &lt;- rnorm(100, mean = 0, sd = 0.2) #Generates 100 error terms\n\n# Generate Y as a noisy sine wave according to equation\ny_vec &lt;- numeric(100) #allocate space\nfor (i in 1:100){\n  y_vec[i]&lt;- sin(x_vec[i]/10) + e_vec[i] #equation given\n}"
  },
  {
    "objectID": "Prac2.html#implementing-the-lowess-algorithm",
    "href": "Prac2.html#implementing-the-lowess-algorithm",
    "title": "Prac2",
    "section": "",
    "text": "customLowess &lt;- function(x, y, f){\n  # Data \n  n &lt;- length(x)\n  # Span\n  k &lt;- f*n\n  # Smoothed weights\n  returnable &lt;- numeric(n)\n  \n  for(i in 1:n){ #repeat for all x_i\n    distances &lt;- abs(x-x[i]) #get distances of neighbours\n    neighbours_ordered &lt;- order(distances)[1:k] #stores k closest neighbours indices\n    d_max &lt;- distances[neighbours_ordered[k]] #furthest = last in ordered vector\n    weights &lt;- (1-(distances[neighbours_ordered]/d_max)^3)^3 #Tricube\n    \n    #Weighted Regression\n    neighbour_x &lt;- x[neighbours_ordered]\n    neighbour_y &lt;- y[neighbours_ordered]\n    weight_diag &lt;- diag(weights) #makes a diagonal matrix of the weights\n    design_matrix &lt;- cbind(1, neighbour_x)\n    beta &lt;- solve(\n      t(design_matrix)%*%weight_diag%*%design_matrix\n      )%*%\n      t(design_matrix)%*%weight_diag%*%neighbour_y\n    \n    #Smoothed value\n    smoothed_y &lt;- beta[1]+beta[2]*x[i]\n    returnable[i] &lt;- smoothed_y\n  }\n  \n  return(returnable)\n}\n\nsmoothed &lt;- numeric(100)\nsmoothed &lt;- customLowess(x_vec, y_vec, 0.1)"
  },
  {
    "objectID": "Prac2.html#comparison-with-built-in-lowess",
    "href": "Prac2.html#comparison-with-built-in-lowess",
    "title": "Prac2",
    "section": "",
    "text": "I made use of the ggplot2 package to create plots of the data and compare them side-by-side as seen below. I needed to install and use the patchwork package for the quarto rendering.\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\nlibrary(ggplot2)\ninstall.packages(\"patchwork\")\n\npackage 'patchwork' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\fabio\\AppData\\Local\\Temp\\RtmpmEDkZY\\downloaded_packages\n\nlibrary(patchwork)\n\nsmoothed &lt;- numeric(100)\nsmoothed &lt;- customLowess(x_vec, y_vec, 0.1)\n\n#makes it the smoothed y-values as it returns \n#both x and y by default\nbuilt_in_smooth &lt;- lowess(x_vec, y_vec, 0.1, 0)$y \n\n#Plot CUSTOM\ncustom_plot &lt;- ggplot(\n  data.frame(\n    x_vec = x_vec, \n    y_vec = y_vec, \n    smoothed = smoothed), \n  aes(x = x_vec, y = y_vec)) + \n  geom_point(color = \"gray\") + \n  geom_line(\n    aes(y = smoothed), \n    color = \"red\", \n    lwd = 1) + ggtitle(\"Custom Function\")\n\n#Plot BUILT-IN\nbuilt_in_plot &lt;- ggplot(\n  data.frame(\n    x_vec = x_vec, \n    y_vec = y_vec, \n    built_in_smooth = built_in_smooth), \n  aes(x = x_vec, y = y_vec)) + \n  geom_point(color = \"gray\") + \n  geom_line(\n    aes(y = built_in_smooth), \n    color = \"green\", \n    lwd = 1) + ggtitle(\"Built-in Function\")\n\nfinal_plot &lt;- custom_plot + built_in_plot #displays them side-by-side\nfinal_plot\n\n\n\n\n\n\n\n\nAs you can see, the 2 plots are identical. Thus our custom function is working correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peer-Reviewed hand-in",
    "section": "",
    "text": "Hi! So, you’re going to be peer-reviewing my practicals from the first week of Statistical Computing. This page contains links to all of the practicals just to make things easier for you! :)\n\n\n\nThese are all my practicals\n\nPractical 1 - Data Analysis and Custom Linear Modelling Function\nPractical 2 - Custom Lowess Smoothing Function\nPractical 3 (Day 4) - Tidyverse and dplyr dataset manipulation"
  },
  {
    "objectID": "index.html#available-practicals",
    "href": "index.html#available-practicals",
    "title": "Peer-Reviewed hand-in",
    "section": "",
    "text": "These are all my practicals\n\nPractical 1 - Data Analysis and Custom Linear Modelling Function\nPractical 2 - Custom Lowess Smoothing Function\nPractical 3 (Day 4) - Tidyverse and dplyr dataset manipulation"
  },
  {
    "objectID": "PracD4.html",
    "href": "PracD4.html",
    "title": "PracD4",
    "section": "",
    "text": "Using tidyr and dplyr to manipulate data frames\n\n\n\nAn alternative way to display a data set without using the print() function is to transform the data set into a tibble and just call the tibble\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\ninstall.packages(\"tidyverse\")\n\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\fabio\\AppData\\Local\\Temp\\RtmpIBfaxc\\downloaded_packages\n\ninstall.packages(\"nycflights13\")\n\npackage 'nycflights13' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\fabio\\AppData\\Local\\Temp\\RtmpIBfaxc\\downloaded_packages\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nflights_tib &lt;- as_tibble(flights)\nflights_tib\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nThe given code’s purpose was to determine the mean distance and stand deviation of distance for each carrier (airline) in the first month of the year, and then display the airlines with these statistics in ascending order of mean distance.\n\ndist_tbl &lt;- flights |&gt; filter(month == 1) |&gt; group_by(carrier) |&gt;\n  summarise(\n    mean_distance = mean(distance, na.rm = TRUE),\n    sd_distance = sd(distance, na.rm= TRUE)\n    ) |&gt; arrange(mean_distance) #does everything besides displaying \ndist_tbl #displays\n\n# A tibble: 16 × 3\n   carrier mean_distance sd_distance\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 YV               229          0  \n 2 9E               476.       334. \n 3 EV               522.       294. \n 4 US               536.       553. \n 5 MQ               566.       223. \n 6 FL               691.       142. \n 7 OO               733         NA  \n 8 WN               942.       496. \n 9 B6              1062.       681. \n10 DL              1220.       644. \n11 AA              1350.       626. \n12 UA              1462.       778. \n13 F9              1620          0  \n14 AS              2402          0  \n15 VX              2495.        98.2\n16 HA              4983          0  \n\n\n\n\n\n\n\nFor a carrier to have a standard deviation of 0, that would imply that all trips are the same distance, which means that the carriers only fly 1 route that month. The carriers which have a standard deviation of 0 are:\n\nYV\nF9\nAS\nHA\n\n\ncarriers_zero &lt;- c(\"YV\", \"F9\", \"AS\", \"HA\")\nflights |&gt; filter(\n  month == 1, \n  carrier %in% carriers_zero\n  ) |&gt; group_by(carrier) |&gt; \n  summarize(\n    num_unique_routes = n_distinct(paste(origin, dest, sep = \"-\")), \n    route = unique(paste(origin, dest, sep = \"-\")))\n\n# A tibble: 4 × 3\n  carrier num_unique_routes route  \n  &lt;chr&gt;               &lt;int&gt; &lt;chr&gt;  \n1 AS                      1 EWR-SEA\n2 F9                      1 LGA-DEN\n3 HA                      1 JFK-HNL\n4 YV                      1 LGA-IAD\n\n\nThis code displays the carriers which have 1 route in the 1st month (1 unique route). It confirms the hypothesis that the carriers with a standard deviation of 0 only have 1 route, travelling the same distance.\n\n\n\nNA is just a representation of missing values. This means that there is not sufficient data to determine the standard deviation of the carrier. This would imply that either there were no flights in the first month or there was only 1 flight in the first month, thus no standard deviation could be determined. The carrier with NA standard deviation is OO.\n\nflights |&gt; filter(\n  month == 1, \n  carrier==\"OO\"\n  ) |&gt; group_by(carrier) |&gt; count(\n    carrier, \n    name = \"number_of_flights\")\n\n# A tibble: 1 × 2\n# Groups:   carrier [1]\n  carrier number_of_flights\n  &lt;chr&gt;               &lt;int&gt;\n1 OO                      1\n\n\nThis code determines the number of flights that carrier OO does. It shows that OO only has 1 flight, thus no standard deviation could be calculated.\n\n\n\n\nThe point of this exercise was to create a data frame that would display the average departure delay of each carrier in each month as rows using dplyr and tidyr. The tibble must be grouped by the month and carrier, and uses the summarize() function to obtain the average delays of each carrier for each month. The tibble was then pivoted to place the average delays as the rows.\n\n#Construct the grouped column tibble\ndelay_tbl &lt;- flights |&gt; group_by(month, carrier) |&gt;\n  summarise(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    .groups = \"drop\") #must drop groups at end or else it's a weird tibble\n\n#Create the data frame with the average delays as rows\ndelay_wide &lt;- delay_tbl |&gt; pivot_wider(\n  names_from = carrier, \n  values_from = avg_delay)\ndelay_wide #display\n\n# A tibble: 12 × 17\n   month  `9E`    AA     AS    B6    DL    EV    F9    FL    HA    MQ    OO\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 16.9   6.93  7.35   9.49  3.85 24.2  10     1.97 54.4   6.49 67   \n 2     2 16.5   8.28  0.722 13.8   5.54 21.5  29.8   5.18 17.4   8.09 NA   \n 3     3 13.4   8.70  8.42  14.2   9.93 26.2  16.8  17.3   1.16  7.19 NA   \n 4     4 13.6  11.7  11.3   15.2   8.17 22.8  24.6  13.1  -2.1  13.7  NA   \n 5     5 22.7   9.66  6.77   9.78  9.74 20.2  35.9  19.2  -1.45 13.9  NA   \n 6     6 29.0  14.6  13.1   20.4  18.7  25.5  29.4  38.8   1.47 20.8  61   \n 7     7 31.4  12.1   2.42  24.9  20.6  26.5  31.8  41.2  -1.71 20.7  NA   \n 8     8 17.3   7.17  2.87  15.7   9.85 16.3  22.2  23.4   1.68 10.1  64   \n 9     9  7.75  5.69 -4.52   6.63  5.53  8.24  8.26 16.9  -5.44  5.35 -4.94\n10    10  9.33  3.00  0.677  2.96  3.42 13.4   9.70 13.7  -5.10  4.48 NA   \n11    11  7.56  3.10  3.08   3.52  2.85  9.83 13.5  16.9  -5.44  3.28  0.8 \n12    12 19.8  11.7  18.0   17.0  10.8  27.9  13.1  26.1  -3.14 12.7  NA   \n# ℹ 5 more variables: UA &lt;dbl&gt;, US &lt;dbl&gt;, VX &lt;dbl&gt;, WN &lt;dbl&gt;, YV &lt;dbl&gt;\n\n\n\n\n\nA flight is considered recovered if its departure is delayed (dep_delay &gt; 0) and it arrives early or on-time (arr_delay &lt;= 0). In essence, we have to calculate the total number of flights which arrived on time but had their departure delayed, and then divide it by the total number of flights. We also needed to remove all instances of NA values as it would inflate the total number of flights erroneously.\n\nflights |&gt; drop_na(dep_delay, arr_delay) |&gt; \n  summarize(\n    num_flights = n(), \n    num_recovered = sum(dep_delay&gt;0 & arr_delay &lt;=0),\n    proportion_delayed_arrived_on_time = num_recovered/num_flights)\n\n# A tibble: 1 × 3\n  num_flights num_recovered proportion_delayed_arrived_on_time\n        &lt;int&gt;         &lt;int&gt;                              &lt;dbl&gt;\n1      327346         35442                              0.108\n\n\nFrom the code, we can see that the proportion of flights which recovered is 0.1082708, i.e. approximately 10.83% of flights.\n\n\n\nAll the sub-questions will consist of us performing analysis on routes that have multiple carriers flying them, so we will create a data frame consisting of this data.\n\nmulti_carrier_routes &lt;- flights |&gt; \n  group_by(origin, dest) |&gt; \n  filter(n_distinct(carrier) &gt; 1) |&gt; \n  ungroup()\nmulti_carrier_routes\n\n# A tibble: 276,735 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      542            540         2      923            850\n 2  2013     1     1      554            600        -6      812            837\n 3  2013     1     1      554            558        -4      740            728\n 4  2013     1     1      555            600        -5      913            854\n 5  2013     1     1      557            600        -3      709            723\n 6  2013     1     1      557            600        -3      838            846\n 7  2013     1     1      558            600        -2      753            745\n 8  2013     1     1      558            600        -2      853            856\n 9  2013     1     1      558            600        -2      924            917\n10  2013     1     1      558            600        -2      923            937\n# ℹ 276,725 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nroutes &lt;- multi_carrier_routes |&gt; \n  select(origin, dest) |&gt; \n  distinct(origin, dest)\nroutes\n\n# A tibble: 128 × 2\n   origin dest \n   &lt;chr&gt;  &lt;chr&gt;\n 1 JFK    MIA  \n 2 LGA    ATL  \n 3 EWR    ORD  \n 4 EWR    FLL  \n 5 LGA    IAD  \n 6 JFK    MCO  \n 7 LGA    ORD  \n 8 JFK    TPA  \n 9 JFK    LAX  \n10 EWR    SFO  \n# ℹ 118 more rows\n\n\n\n\n\n\n#The more comprehensive data frame\narr_delay_carrier &lt;- multi_carrier_routes |&gt;\n  group_by(origin, dest, carrier) |&gt;\n  summarize(\n    avg_arr_delay = mean(arr_delay, na.rm = TRUE),\n    .groups = \"drop\") |&gt; \n  left_join(airlines, by = \"carrier\")\narr_delay_carrier\n\n# A tibble: 343 × 5\n   origin dest  carrier avg_arr_delay name                    \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;                   \n 1 EWR    ATL   9E              -6.25 Endeavor Air Inc.       \n 2 EWR    ATL   DL              10.0  Delta Air Lines Inc.    \n 3 EWR    ATL   EV              19.5  ExpressJet Airlines Inc.\n 4 EWR    ATL   UA              10.5  United Air Lines Inc.   \n 5 EWR    AUS   UA               4.28 United Air Lines Inc.   \n 6 EWR    AUS   WN             -11.2  Southwest Airlines Co.  \n 7 EWR    BDL   EV               6.78 ExpressJet Airlines Inc.\n 8 EWR    BDL   UA              22.6  United Air Lines Inc.   \n 9 EWR    BNA   EV              17.7  ExpressJet Airlines Inc.\n10 EWR    BNA   WN              -2.13 Southwest Airlines Co.  \n# ℹ 333 more rows\n\n\n\n# The more selective data frame\nairline_names &lt;- arr_delay_carrier |&gt; \n  select(name, avg_arr_delay, origin, dest) \nairline_names\n\n# A tibble: 343 × 4\n   name                     avg_arr_delay origin dest \n   &lt;chr&gt;                            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 Endeavor Air Inc.                -6.25 EWR    ATL  \n 2 Delta Air Lines Inc.             10.0  EWR    ATL  \n 3 ExpressJet Airlines Inc.         19.5  EWR    ATL  \n 4 United Air Lines Inc.            10.5  EWR    ATL  \n 5 United Air Lines Inc.             4.28 EWR    AUS  \n 6 Southwest Airlines Co.          -11.2  EWR    AUS  \n 7 ExpressJet Airlines Inc.          6.78 EWR    BDL  \n 8 United Air Lines Inc.            22.6  EWR    BDL  \n 9 ExpressJet Airlines Inc.         17.7  EWR    BNA  \n10 Southwest Airlines Co.           -2.13 EWR    BNA  \n# ℹ 333 more rows\n\n\nThe airline names needed to be attached to the data frame with left_join().\n\n\n\n\n#This will show you more data\nbest_worst_delay &lt;- arr_delay_carrier |&gt;\n  group_by(origin, dest) |&gt; \n  summarize(\n    best_delay = min(avg_arr_delay, na.rm = TRUE),\n    worst_delay = max(avg_arr_delay, na.rm = TRUE),\n    best_airline = name[which.min(avg_arr_delay)],\n    worst_airline = name[which.max(avg_arr_delay)],\n    delay_diff = worst_delay - best_delay,\n    .groups = \"drop\")\nbest_worst_delay\n\n# A tibble: 128 × 7\n   origin dest  best_delay worst_delay best_airline     worst_airline delay_diff\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;dbl&gt;\n 1 EWR    ATL       -6.25        19.5  Endeavor Air In… ExpressJet A…      25.8 \n 2 EWR    AUS      -11.2          4.28 Southwest Airli… United Air L…      15.5 \n 3 EWR    BDL        6.78        22.6  ExpressJet Airl… United Air L…      15.8 \n 4 EWR    BNA       -2.13        17.7  Southwest Airli… ExpressJet A…      19.8 \n 5 EWR    BOS       -4.01         6.87 ExpressJet Airl… JetBlue Airw…      10.9 \n 6 EWR    BWI        5.95        20.1  Southwest Airli… ExpressJet A…      14.1 \n 7 EWR    CHS      -14           16.2  United Air Line… ExpressJet A…      30.2 \n 8 EWR    CLE       -3.71         5.97 ExpressJet Airl… United Air L…       9.68\n 9 EWR    CLT        0.920       20.5  US Airways Inc.  ExpressJet A…      19.6 \n10 EWR    CVG        1.40        21.2  Endeavor Air In… ExpressJet A…      19.8 \n# ℹ 118 more rows\n\n\n\n#This shows you the more specific data\nbest_worst_airline &lt;- best_worst_delay |&gt; \n  select(origin, dest, best_airline, worst_airline)\nbest_worst_airline\n\n# A tibble: 128 × 4\n   origin dest  best_airline             worst_airline           \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                    &lt;chr&gt;                   \n 1 EWR    ATL   Endeavor Air Inc.        ExpressJet Airlines Inc.\n 2 EWR    AUS   Southwest Airlines Co.   United Air Lines Inc.   \n 3 EWR    BDL   ExpressJet Airlines Inc. United Air Lines Inc.   \n 4 EWR    BNA   Southwest Airlines Co.   ExpressJet Airlines Inc.\n 5 EWR    BOS   ExpressJet Airlines Inc. JetBlue Airways         \n 6 EWR    BWI   Southwest Airlines Co.   ExpressJet Airlines Inc.\n 7 EWR    CHS   United Air Lines Inc.    ExpressJet Airlines Inc.\n 8 EWR    CLE   ExpressJet Airlines Inc. United Air Lines Inc.   \n 9 EWR    CLT   US Airways Inc.          ExpressJet Airlines Inc.\n10 EWR    CVG   Endeavor Air Inc.        ExpressJet Airlines Inc.\n# ℹ 118 more rows\n\n\nI decided to also calculate the difference in delays between the best and worst airlines for each route as it would be used later.\n\n\n\nThis will make use of the above data frame “best_worst_delay”.\n\ngreatest_diff &lt;- best_worst_delay |&gt; \n  arrange(desc(delay_diff)) |&gt; \n  filter(delay_diff == max(delay_diff, na.rm = TRUE)) |&gt;\n  select(\n    origin, \n    dest, \n    best_airline, \n    worst_airline, \n    delay_diff)\ngreatest_diff\n\n# A tibble: 1 × 5\n  origin dest  best_airline      worst_airline            delay_diff\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;                         &lt;dbl&gt;\n1 JFK    ATL   Endeavor Air Inc. ExpressJet Airlines Inc.       127.\n\n\n\nIt should be noted that this could also be achieved by slicing the ordered data set at the first row, as that will show the maximum delay difference (this is left as an exercise to the reader).\n\n\n\n\nPotential reasons for this delay difference could be due to insufficient data (i.e. one airline has made a sizable number of trips along the route and the other has only a few very bad/good trips which will skew the data). Other reasons would be unsubstantiated as we do not have data for things such as weather or cleaning times.\n\nmulti_carrier_routes |&gt; \n  left_join(airlines, by = \"carrier\") |&gt;\n  group_by(origin, dest, carrier ,name) |&gt;\n  summarize(flight_count = n(), .groups = \"drop\") |&gt;\n  filter(\n    origin == \"JFK\", \n    dest == \"ATL\", \n    carrier %in% c(\"9E\", \"EV\"))\n\n# A tibble: 2 × 5\n  origin dest  carrier name                     flight_count\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                           &lt;int&gt;\n1 JFK    ATL   9E      Endeavor Air Inc.                  55\n2 JFK    ATL   EV      ExpressJet Airlines Inc.            1\n\n\nThis code isolates the data of the airlines of interest in order to count how many flights were made on the route with the greatest delay difference. We can see that ExpressJet Airlines Inc. has only made 1 flight along that route which means its average delay is the value of the delay on its only flight, skewing the data.\n\n\n\n\nIn order to find missing entries, typos and inconsistencies in the data, we first have to load the data. I did this by creating a data object and assigning the data set to it.\n\n\n\nmissing_values &lt;- weird_data |&gt; \n  summarise(\n    across(everything(), \n           ~sum(is.na(.)), \n           .names = \"missing_{.col}\"))\nmissing_values\n\n# A tibble: 1 × 15\n  missing_id missing_age missing_gender missing_height missing_weight\n       &lt;int&gt;       &lt;int&gt;          &lt;int&gt;          &lt;int&gt;          &lt;int&gt;\n1          0           0              0              2              0\n# ℹ 10 more variables: missing_blood_type &lt;int&gt;, missing_disease_status &lt;int&gt;,\n#   missing_cholesterol &lt;int&gt;, missing_glucose &lt;int&gt;, missing_smoker &lt;int&gt;,\n#   missing_exercise &lt;int&gt;, missing_income &lt;int&gt;, missing_education &lt;int&gt;,\n#   missing_region &lt;int&gt;, missing_marital_status &lt;int&gt;\n\nmissing_values |&gt; \n  pivot_longer(\n    everything(), \n    names_to = \"column\", \n    values_to = \"missing_count\") |&gt;\n  filter(missing_count&gt;0)\n\n# A tibble: 2 × 2\n  column          missing_count\n  &lt;chr&gt;                   &lt;int&gt;\n1 missing_height              2\n2 missing_glucose             3\n\n\nThis code checks all columns for NA values. The ~ makes this an anonymous function so we do not need to make the function, and the “.” is what tells the program to check the entire column. It then displays only the columns with missing values.\n\n\n\nTo determine the number of typos, I made use of the viewcols() function that comes with the UtilsDataRSV package.\n\nUtilsDataRSV::view_cols(weird_data)\n\n[1] \"id\"\n [1] \"id_25\" \"id_47\" \"id_21\" \"id_20\" \"id_8\"  \"id_23\" \"id_22\" \"id_4\"  \"id_37\"\n[10] \"id_31\" \"id_33\" \"id_17\" \"id_50\" \"id_10\" \"id_45\" \"id_44\" \"id_49\" \"id_7\" \n[19] \"id_14\" \"id_28\"\n[1] \"30 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"age\"\n[1] 76 70 50 28 42\n[1] \"_____________________\"\n[1] \"gender\"\n[1] \"femal\"  \"female\" \"male\"  \n[1] \"_____________________\"\n[1] \"height\"\n[1] 170.5 161.0 183.6 186.9    NA\n[1] \"_____________________\"\n[1] \"weight\"\n[1] 57.6 69.5 61.1 63.7 60.8\n[1] \"_____________________\"\n[1] \"blood_type\"\n[1] \"A\"  \"O\"  \"AB\" \"B\" \n[1] \"_____________________\"\n[1] \"disease_status\"\n[1] \"diseased\" \"Healthy\"  \"healthy\" \n[1] \"_____________________\"\n[1] \"cholesterol\"\n[1] 153 212 184 166 247\n[1] \"_____________________\"\n[1] \"glucose\"\n[1] 89 77 93 87 NA\n[1] \"_____________________\"\n[1] \"smoker\"\n[1] \"no\"  \"yes\"\n[1] \"_____________________\"\n[1] \"exercise\"\n[1] \"occasional\" \"regular\"    \"none\"      \n[1] \"_____________________\"\n[1] \"income\"\n[1] 84820 22607 68275 84173 78611\n[1] \"_____________________\"\n[1] \"education\"\n[1] \"highschool\" \"bachelor\"   \"PhD\"        \"master\"    \n[1] \"_____________________\"\n[1] \"region\"\n[1] \"West\"  \"East\"  \"South\" \"North\"\n[1] \"_____________________\"\n[1] \"marital_status\"\n[1] \"single\"   \"divorced\" \"married\"  \"widowed\" \n[1] \"_____________________\"\n\n\nWe can see that there are 2 typos:\n\n“femal” in the gender column\n“Healthy” in the disease_status column"
  },
  {
    "objectID": "PracD4.html#displaying-the-flights-data-set",
    "href": "PracD4.html#displaying-the-flights-data-set",
    "title": "PracD4",
    "section": "",
    "text": "An alternative way to display a data set without using the print() function is to transform the data set into a tibble and just call the tibble\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\ninstall.packages(\"tidyverse\")\n\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\fabio\\AppData\\Local\\Temp\\RtmpIBfaxc\\downloaded_packages\n\ninstall.packages(\"nycflights13\")\n\npackage 'nycflights13' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\fabio\\AppData\\Local\\Temp\\RtmpIBfaxc\\downloaded_packages\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nflights_tib &lt;- as_tibble(flights)\nflights_tib\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "PracD4.html#rewriting-code-using-dplyr-and-the-pipe",
    "href": "PracD4.html#rewriting-code-using-dplyr-and-the-pipe",
    "title": "PracD4",
    "section": "",
    "text": "The given code’s purpose was to determine the mean distance and stand deviation of distance for each carrier (airline) in the first month of the year, and then display the airlines with these statistics in ascending order of mean distance.\n\ndist_tbl &lt;- flights |&gt; filter(month == 1) |&gt; group_by(carrier) |&gt;\n  summarise(\n    mean_distance = mean(distance, na.rm = TRUE),\n    sd_distance = sd(distance, na.rm= TRUE)\n    ) |&gt; arrange(mean_distance) #does everything besides displaying \ndist_tbl #displays\n\n# A tibble: 16 × 3\n   carrier mean_distance sd_distance\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 YV               229          0  \n 2 9E               476.       334. \n 3 EV               522.       294. \n 4 US               536.       553. \n 5 MQ               566.       223. \n 6 FL               691.       142. \n 7 OO               733         NA  \n 8 WN               942.       496. \n 9 B6              1062.       681. \n10 DL              1220.       644. \n11 AA              1350.       626. \n12 UA              1462.       778. \n13 F9              1620          0  \n14 AS              2402          0  \n15 VX              2495.        98.2\n16 HA              4983          0"
  },
  {
    "objectID": "PracD4.html#standard-deviation-explanation",
    "href": "PracD4.html#standard-deviation-explanation",
    "title": "PracD4",
    "section": "",
    "text": "For a carrier to have a standard deviation of 0, that would imply that all trips are the same distance, which means that the carriers only fly 1 route that month. The carriers which have a standard deviation of 0 are:\n\nYV\nF9\nAS\nHA\n\n\ncarriers_zero &lt;- c(\"YV\", \"F9\", \"AS\", \"HA\")\nflights |&gt; filter(\n  month == 1, \n  carrier %in% carriers_zero\n  ) |&gt; group_by(carrier) |&gt; \n  summarize(\n    num_unique_routes = n_distinct(paste(origin, dest, sep = \"-\")), \n    route = unique(paste(origin, dest, sep = \"-\")))\n\n# A tibble: 4 × 3\n  carrier num_unique_routes route  \n  &lt;chr&gt;               &lt;int&gt; &lt;chr&gt;  \n1 AS                      1 EWR-SEA\n2 F9                      1 LGA-DEN\n3 HA                      1 JFK-HNL\n4 YV                      1 LGA-IAD\n\n\nThis code displays the carriers which have 1 route in the 1st month (1 unique route). It confirms the hypothesis that the carriers with a standard deviation of 0 only have 1 route, travelling the same distance.\n\n\n\nNA is just a representation of missing values. This means that there is not sufficient data to determine the standard deviation of the carrier. This would imply that either there were no flights in the first month or there was only 1 flight in the first month, thus no standard deviation could be determined. The carrier with NA standard deviation is OO.\n\nflights |&gt; filter(\n  month == 1, \n  carrier==\"OO\"\n  ) |&gt; group_by(carrier) |&gt; count(\n    carrier, \n    name = \"number_of_flights\")\n\n# A tibble: 1 × 2\n# Groups:   carrier [1]\n  carrier number_of_flights\n  &lt;chr&gt;               &lt;int&gt;\n1 OO                      1\n\n\nThis code determines the number of flights that carrier OO does. It shows that OO only has 1 flight, thus no standard deviation could be calculated."
  },
  {
    "objectID": "PracD4.html#data-frame-with-average-departure-delay-rows",
    "href": "PracD4.html#data-frame-with-average-departure-delay-rows",
    "title": "PracD4",
    "section": "",
    "text": "The point of this exercise was to create a data frame that would display the average departure delay of each carrier in each month as rows using dplyr and tidyr. The tibble must be grouped by the month and carrier, and uses the summarize() function to obtain the average delays of each carrier for each month. The tibble was then pivoted to place the average delays as the rows.\n\n#Construct the grouped column tibble\ndelay_tbl &lt;- flights |&gt; group_by(month, carrier) |&gt;\n  summarise(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    .groups = \"drop\") #must drop groups at end or else it's a weird tibble\n\n#Create the data frame with the average delays as rows\ndelay_wide &lt;- delay_tbl |&gt; pivot_wider(\n  names_from = carrier, \n  values_from = avg_delay)\ndelay_wide #display\n\n# A tibble: 12 × 17\n   month  `9E`    AA     AS    B6    DL    EV    F9    FL    HA    MQ    OO\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 16.9   6.93  7.35   9.49  3.85 24.2  10     1.97 54.4   6.49 67   \n 2     2 16.5   8.28  0.722 13.8   5.54 21.5  29.8   5.18 17.4   8.09 NA   \n 3     3 13.4   8.70  8.42  14.2   9.93 26.2  16.8  17.3   1.16  7.19 NA   \n 4     4 13.6  11.7  11.3   15.2   8.17 22.8  24.6  13.1  -2.1  13.7  NA   \n 5     5 22.7   9.66  6.77   9.78  9.74 20.2  35.9  19.2  -1.45 13.9  NA   \n 6     6 29.0  14.6  13.1   20.4  18.7  25.5  29.4  38.8   1.47 20.8  61   \n 7     7 31.4  12.1   2.42  24.9  20.6  26.5  31.8  41.2  -1.71 20.7  NA   \n 8     8 17.3   7.17  2.87  15.7   9.85 16.3  22.2  23.4   1.68 10.1  64   \n 9     9  7.75  5.69 -4.52   6.63  5.53  8.24  8.26 16.9  -5.44  5.35 -4.94\n10    10  9.33  3.00  0.677  2.96  3.42 13.4   9.70 13.7  -5.10  4.48 NA   \n11    11  7.56  3.10  3.08   3.52  2.85  9.83 13.5  16.9  -5.44  3.28  0.8 \n12    12 19.8  11.7  18.0   17.0  10.8  27.9  13.1  26.1  -3.14 12.7  NA   \n# ℹ 5 more variables: UA &lt;dbl&gt;, US &lt;dbl&gt;, VX &lt;dbl&gt;, WN &lt;dbl&gt;, YV &lt;dbl&gt;"
  },
  {
    "objectID": "PracD4.html#proportion-of-flights-recovering-from-delays",
    "href": "PracD4.html#proportion-of-flights-recovering-from-delays",
    "title": "PracD4",
    "section": "",
    "text": "A flight is considered recovered if its departure is delayed (dep_delay &gt; 0) and it arrives early or on-time (arr_delay &lt;= 0). In essence, we have to calculate the total number of flights which arrived on time but had their departure delayed, and then divide it by the total number of flights. We also needed to remove all instances of NA values as it would inflate the total number of flights erroneously.\n\nflights |&gt; drop_na(dep_delay, arr_delay) |&gt; \n  summarize(\n    num_flights = n(), \n    num_recovered = sum(dep_delay&gt;0 & arr_delay &lt;=0),\n    proportion_delayed_arrived_on_time = num_recovered/num_flights)\n\n# A tibble: 1 × 3\n  num_flights num_recovered proportion_delayed_arrived_on_time\n        &lt;int&gt;         &lt;int&gt;                              &lt;dbl&gt;\n1      327346         35442                              0.108\n\n\nFrom the code, we can see that the proportion of flights which recovered is 0.1082708, i.e. approximately 10.83% of flights."
  },
  {
    "objectID": "PracD4.html#airline-performance",
    "href": "PracD4.html#airline-performance",
    "title": "PracD4",
    "section": "",
    "text": "All the sub-questions will consist of us performing analysis on routes that have multiple carriers flying them, so we will create a data frame consisting of this data.\n\nmulti_carrier_routes &lt;- flights |&gt; \n  group_by(origin, dest) |&gt; \n  filter(n_distinct(carrier) &gt; 1) |&gt; \n  ungroup()\nmulti_carrier_routes\n\n# A tibble: 276,735 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      542            540         2      923            850\n 2  2013     1     1      554            600        -6      812            837\n 3  2013     1     1      554            558        -4      740            728\n 4  2013     1     1      555            600        -5      913            854\n 5  2013     1     1      557            600        -3      709            723\n 6  2013     1     1      557            600        -3      838            846\n 7  2013     1     1      558            600        -2      753            745\n 8  2013     1     1      558            600        -2      853            856\n 9  2013     1     1      558            600        -2      924            917\n10  2013     1     1      558            600        -2      923            937\n# ℹ 276,725 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n\nroutes &lt;- multi_carrier_routes |&gt; \n  select(origin, dest) |&gt; \n  distinct(origin, dest)\nroutes\n\n# A tibble: 128 × 2\n   origin dest \n   &lt;chr&gt;  &lt;chr&gt;\n 1 JFK    MIA  \n 2 LGA    ATL  \n 3 EWR    ORD  \n 4 EWR    FLL  \n 5 LGA    IAD  \n 6 JFK    MCO  \n 7 LGA    ORD  \n 8 JFK    TPA  \n 9 JFK    LAX  \n10 EWR    SFO  \n# ℹ 118 more rows\n\n\n\n\n\n\n#The more comprehensive data frame\narr_delay_carrier &lt;- multi_carrier_routes |&gt;\n  group_by(origin, dest, carrier) |&gt;\n  summarize(\n    avg_arr_delay = mean(arr_delay, na.rm = TRUE),\n    .groups = \"drop\") |&gt; \n  left_join(airlines, by = \"carrier\")\narr_delay_carrier\n\n# A tibble: 343 × 5\n   origin dest  carrier avg_arr_delay name                    \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;                   \n 1 EWR    ATL   9E              -6.25 Endeavor Air Inc.       \n 2 EWR    ATL   DL              10.0  Delta Air Lines Inc.    \n 3 EWR    ATL   EV              19.5  ExpressJet Airlines Inc.\n 4 EWR    ATL   UA              10.5  United Air Lines Inc.   \n 5 EWR    AUS   UA               4.28 United Air Lines Inc.   \n 6 EWR    AUS   WN             -11.2  Southwest Airlines Co.  \n 7 EWR    BDL   EV               6.78 ExpressJet Airlines Inc.\n 8 EWR    BDL   UA              22.6  United Air Lines Inc.   \n 9 EWR    BNA   EV              17.7  ExpressJet Airlines Inc.\n10 EWR    BNA   WN              -2.13 Southwest Airlines Co.  \n# ℹ 333 more rows\n\n\n\n# The more selective data frame\nairline_names &lt;- arr_delay_carrier |&gt; \n  select(name, avg_arr_delay, origin, dest) \nairline_names\n\n# A tibble: 343 × 4\n   name                     avg_arr_delay origin dest \n   &lt;chr&gt;                            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 Endeavor Air Inc.                -6.25 EWR    ATL  \n 2 Delta Air Lines Inc.             10.0  EWR    ATL  \n 3 ExpressJet Airlines Inc.         19.5  EWR    ATL  \n 4 United Air Lines Inc.            10.5  EWR    ATL  \n 5 United Air Lines Inc.             4.28 EWR    AUS  \n 6 Southwest Airlines Co.          -11.2  EWR    AUS  \n 7 ExpressJet Airlines Inc.          6.78 EWR    BDL  \n 8 United Air Lines Inc.            22.6  EWR    BDL  \n 9 ExpressJet Airlines Inc.         17.7  EWR    BNA  \n10 Southwest Airlines Co.           -2.13 EWR    BNA  \n# ℹ 333 more rows\n\n\nThe airline names needed to be attached to the data frame with left_join().\n\n\n\n\n#This will show you more data\nbest_worst_delay &lt;- arr_delay_carrier |&gt;\n  group_by(origin, dest) |&gt; \n  summarize(\n    best_delay = min(avg_arr_delay, na.rm = TRUE),\n    worst_delay = max(avg_arr_delay, na.rm = TRUE),\n    best_airline = name[which.min(avg_arr_delay)],\n    worst_airline = name[which.max(avg_arr_delay)],\n    delay_diff = worst_delay - best_delay,\n    .groups = \"drop\")\nbest_worst_delay\n\n# A tibble: 128 × 7\n   origin dest  best_delay worst_delay best_airline     worst_airline delay_diff\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;dbl&gt;\n 1 EWR    ATL       -6.25        19.5  Endeavor Air In… ExpressJet A…      25.8 \n 2 EWR    AUS      -11.2          4.28 Southwest Airli… United Air L…      15.5 \n 3 EWR    BDL        6.78        22.6  ExpressJet Airl… United Air L…      15.8 \n 4 EWR    BNA       -2.13        17.7  Southwest Airli… ExpressJet A…      19.8 \n 5 EWR    BOS       -4.01         6.87 ExpressJet Airl… JetBlue Airw…      10.9 \n 6 EWR    BWI        5.95        20.1  Southwest Airli… ExpressJet A…      14.1 \n 7 EWR    CHS      -14           16.2  United Air Line… ExpressJet A…      30.2 \n 8 EWR    CLE       -3.71         5.97 ExpressJet Airl… United Air L…       9.68\n 9 EWR    CLT        0.920       20.5  US Airways Inc.  ExpressJet A…      19.6 \n10 EWR    CVG        1.40        21.2  Endeavor Air In… ExpressJet A…      19.8 \n# ℹ 118 more rows\n\n\n\n#This shows you the more specific data\nbest_worst_airline &lt;- best_worst_delay |&gt; \n  select(origin, dest, best_airline, worst_airline)\nbest_worst_airline\n\n# A tibble: 128 × 4\n   origin dest  best_airline             worst_airline           \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;                    &lt;chr&gt;                   \n 1 EWR    ATL   Endeavor Air Inc.        ExpressJet Airlines Inc.\n 2 EWR    AUS   Southwest Airlines Co.   United Air Lines Inc.   \n 3 EWR    BDL   ExpressJet Airlines Inc. United Air Lines Inc.   \n 4 EWR    BNA   Southwest Airlines Co.   ExpressJet Airlines Inc.\n 5 EWR    BOS   ExpressJet Airlines Inc. JetBlue Airways         \n 6 EWR    BWI   Southwest Airlines Co.   ExpressJet Airlines Inc.\n 7 EWR    CHS   United Air Lines Inc.    ExpressJet Airlines Inc.\n 8 EWR    CLE   ExpressJet Airlines Inc. United Air Lines Inc.   \n 9 EWR    CLT   US Airways Inc.          ExpressJet Airlines Inc.\n10 EWR    CVG   Endeavor Air Inc.        ExpressJet Airlines Inc.\n# ℹ 118 more rows\n\n\nI decided to also calculate the difference in delays between the best and worst airlines for each route as it would be used later.\n\n\n\nThis will make use of the above data frame “best_worst_delay”.\n\ngreatest_diff &lt;- best_worst_delay |&gt; \n  arrange(desc(delay_diff)) |&gt; \n  filter(delay_diff == max(delay_diff, na.rm = TRUE)) |&gt;\n  select(\n    origin, \n    dest, \n    best_airline, \n    worst_airline, \n    delay_diff)\ngreatest_diff\n\n# A tibble: 1 × 5\n  origin dest  best_airline      worst_airline            delay_diff\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;                         &lt;dbl&gt;\n1 JFK    ATL   Endeavor Air Inc. ExpressJet Airlines Inc.       127.\n\n\n\nIt should be noted that this could also be achieved by slicing the ordered data set at the first row, as that will show the maximum delay difference (this is left as an exercise to the reader).\n\n\n\n\nPotential reasons for this delay difference could be due to insufficient data (i.e. one airline has made a sizable number of trips along the route and the other has only a few very bad/good trips which will skew the data). Other reasons would be unsubstantiated as we do not have data for things such as weather or cleaning times.\n\nmulti_carrier_routes |&gt; \n  left_join(airlines, by = \"carrier\") |&gt;\n  group_by(origin, dest, carrier ,name) |&gt;\n  summarize(flight_count = n(), .groups = \"drop\") |&gt;\n  filter(\n    origin == \"JFK\", \n    dest == \"ATL\", \n    carrier %in% c(\"9E\", \"EV\"))\n\n# A tibble: 2 × 5\n  origin dest  carrier name                     flight_count\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                           &lt;int&gt;\n1 JFK    ATL   9E      Endeavor Air Inc.                  55\n2 JFK    ATL   EV      ExpressJet Airlines Inc.            1\n\n\nThis code isolates the data of the airlines of interest in order to count how many flights were made on the route with the greatest delay difference. We can see that ExpressJet Airlines Inc. has only made 1 flight along that route which means its average delay is the value of the delay on its only flight, skewing the data."
  },
  {
    "objectID": "PracD4.html#missing-entries-typos-and-inconsistencies",
    "href": "PracD4.html#missing-entries-typos-and-inconsistencies",
    "title": "PracD4",
    "section": "",
    "text": "In order to find missing entries, typos and inconsistencies in the data, we first have to load the data. I did this by creating a data object and assigning the data set to it.\n\n\n\nmissing_values &lt;- weird_data |&gt; \n  summarise(\n    across(everything(), \n           ~sum(is.na(.)), \n           .names = \"missing_{.col}\"))\nmissing_values\n\n# A tibble: 1 × 15\n  missing_id missing_age missing_gender missing_height missing_weight\n       &lt;int&gt;       &lt;int&gt;          &lt;int&gt;          &lt;int&gt;          &lt;int&gt;\n1          0           0              0              2              0\n# ℹ 10 more variables: missing_blood_type &lt;int&gt;, missing_disease_status &lt;int&gt;,\n#   missing_cholesterol &lt;int&gt;, missing_glucose &lt;int&gt;, missing_smoker &lt;int&gt;,\n#   missing_exercise &lt;int&gt;, missing_income &lt;int&gt;, missing_education &lt;int&gt;,\n#   missing_region &lt;int&gt;, missing_marital_status &lt;int&gt;\n\nmissing_values |&gt; \n  pivot_longer(\n    everything(), \n    names_to = \"column\", \n    values_to = \"missing_count\") |&gt;\n  filter(missing_count&gt;0)\n\n# A tibble: 2 × 2\n  column          missing_count\n  &lt;chr&gt;                   &lt;int&gt;\n1 missing_height              2\n2 missing_glucose             3\n\n\nThis code checks all columns for NA values. The ~ makes this an anonymous function so we do not need to make the function, and the “.” is what tells the program to check the entire column. It then displays only the columns with missing values.\n\n\n\nTo determine the number of typos, I made use of the viewcols() function that comes with the UtilsDataRSV package.\n\nUtilsDataRSV::view_cols(weird_data)\n\n[1] \"id\"\n [1] \"id_25\" \"id_47\" \"id_21\" \"id_20\" \"id_8\"  \"id_23\" \"id_22\" \"id_4\"  \"id_37\"\n[10] \"id_31\" \"id_33\" \"id_17\" \"id_50\" \"id_10\" \"id_45\" \"id_44\" \"id_49\" \"id_7\" \n[19] \"id_14\" \"id_28\"\n[1] \"30 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"age\"\n[1] 76 70 50 28 42\n[1] \"_____________________\"\n[1] \"gender\"\n[1] \"femal\"  \"female\" \"male\"  \n[1] \"_____________________\"\n[1] \"height\"\n[1] 170.5 161.0 183.6 186.9    NA\n[1] \"_____________________\"\n[1] \"weight\"\n[1] 57.6 69.5 61.1 63.7 60.8\n[1] \"_____________________\"\n[1] \"blood_type\"\n[1] \"A\"  \"O\"  \"AB\" \"B\" \n[1] \"_____________________\"\n[1] \"disease_status\"\n[1] \"diseased\" \"Healthy\"  \"healthy\" \n[1] \"_____________________\"\n[1] \"cholesterol\"\n[1] 153 212 184 166 247\n[1] \"_____________________\"\n[1] \"glucose\"\n[1] 89 77 93 87 NA\n[1] \"_____________________\"\n[1] \"smoker\"\n[1] \"no\"  \"yes\"\n[1] \"_____________________\"\n[1] \"exercise\"\n[1] \"occasional\" \"regular\"    \"none\"      \n[1] \"_____________________\"\n[1] \"income\"\n[1] 84820 22607 68275 84173 78611\n[1] \"_____________________\"\n[1] \"education\"\n[1] \"highschool\" \"bachelor\"   \"PhD\"        \"master\"    \n[1] \"_____________________\"\n[1] \"region\"\n[1] \"West\"  \"East\"  \"South\" \"North\"\n[1] \"_____________________\"\n[1] \"marital_status\"\n[1] \"single\"   \"divorced\" \"married\"  \"widowed\" \n[1] \"_____________________\"\n\n\nWe can see that there are 2 typos:\n\n“femal” in the gender column\n“Healthy” in the disease_status column"
  }
]